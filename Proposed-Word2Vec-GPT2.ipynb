{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Word2Vec-GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification, logging\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim.downloader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import datasets\n",
    "import string\n",
    "import evaluate\n",
    "import csv \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "logging.set_verbosity_error()\n",
    "logging.set_verbosity_warning()\n",
    "HF_HUB_DISABLE_SYMLINKS_WARNING = True\n",
    "\n",
    "import datetime\n",
    "t_delta = datetime.timedelta(hours=9)\n",
    "JST = datetime.timezone(t_delta, 'JST')\n",
    "now = datetime.datetime.now(JST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "MODEL = \"gpt2\"\n",
    "SAVED_MODEL = \"../model/Proposed-Word2Vec-GPT2_\"+str(now.strftime('%Y%m%d%H%M%S'))\n",
    "THRESHOLD = 0.05\n",
    "MAXLEN_GET_PSEUDO = 3000\n",
    "EPOCH = 1\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/Proposed-Word2Vec-GPT2_20221125150654\n"
     ]
    }
   ],
   "source": [
    "print(SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "def preprocessing(text):\n",
    "    # 括弧内文章の削除\n",
    "    text = re.sub(r'\\(.*\\)',' ',text)\n",
    "    text = re.sub(r'\\[.*\\]',' ',text)\n",
    "    text = re.sub(r'\\<.*\\>',' ',text)\n",
    "    text = re.sub(r'\\{.*\\}',' ',text)\n",
    "    # 記号文字の削除\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    # スペースの調整\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 19007.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# 20 newsgroups datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset=\"all\")\n",
    "newsgroups_datasets = list()\n",
    "\n",
    "# example ------------------------------------------------\n",
    "for texts in tqdm(newsgroups.data[:1000]):\n",
    "  texts = texts.split(\"\\n\\n\")\n",
    "  texts = \" \".join(texts[1:])\n",
    "  newsgroups_datasets.append(preprocessing(texts))\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# for texts in tqdm(newsgroups.data):\n",
    "#   texts = texts.split(\"\\n\\n\")\n",
    "#   texts = \" \".join(texts[1:])\n",
    "#   newsgroups_datasets.append(preprocessing(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 40543.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# yahoo topic datasets\n",
    "with open('../data/topic/train_pu_half_v0.txt','r',encoding='utf-8') as f:\n",
    "    texts_v0 = f.read()\n",
    "with open('../data/topic/train_pu_half_v1.txt','r',encoding='utf-8') as f:\n",
    "    texts_v1 = f.read()\n",
    "texts = texts_v0 + texts_v1\n",
    "topic_datasets = list()\n",
    "\n",
    "# example ----------------------------------------------\n",
    "for label_text in tqdm(texts.splitlines()[:1000]):\n",
    "  _, text = label_text.split(\"\\t\")\n",
    "  topic_datasets.append(preprocessing(text))\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# for label_text in tqdm(texts.splitlines()):\n",
    "#   _, text = label_text.split(\"\\t\")\n",
    "#   topic_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 20006.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# reuters datasets\n",
    "with open(\"../data/reuter/sourceall.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  reuter = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "# example -----------------------------------\n",
    "reuter = reuter[:100]\n",
    "# -------------------------------------------\n",
    "\n",
    "reuters_datasets = list()\n",
    "for label_text in tqdm(reuter):\n",
    "  _, text = label_text.split(\"\\t\")\n",
    "  reuters_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 50021.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# dbpedia datasets train\n",
    "with open('../data/dbpedia_csv/train.csv','r',encoding='utf-8') as f:\n",
    "    reader = [r for r in csv.reader(f)]\n",
    "    \n",
    "# example -------------------\n",
    "reader = reader[:100]\n",
    "#----------------------------\n",
    "\n",
    "dbpedia_train_datasets = list()\n",
    "for _, auth, text in tqdm(reader):\n",
    "    text = text.replace(auth,'')\n",
    "    dbpedia_train_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbpedia classes\n",
    "with open(\"../data/dbpedia_csv/classes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  classes = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_texts = newsgroups_datasets + topic_datasets + reuters_datasets + dbpedia_train_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "def w2v_avg_vector(sentence):\n",
    "  vector = np.zeros((300,), dtype=\"float32\")\n",
    "  count = 0\n",
    "  for word in sentence.split():\n",
    "    try:\n",
    "      vector = np.add(vector, word2vec[word])\n",
    "      count += 1\n",
    "    except:\n",
    "      pass\n",
    "  if count > 0:\n",
    "    vector = np.divide(vector, len(word))\n",
    "  return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_vector = list()\n",
    "for cls in classes:\n",
    "  classes_vector.append(w2v_avg_vector(cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2200/2200 [00:00<00:00, 2834.45it/s]\n"
     ]
    }
   ],
   "source": [
    "diff_datasets = {i:[] for i in range(len(classes))}\n",
    "for texts in tqdm(datasets_texts):\n",
    "  texts_vector = w2v_avg_vector(texts)\n",
    "  similarity = cosine_similarity([texts_vector], classes_vector)[0]\n",
    "  sim_argsorted = np.argsort(similarity)\n",
    "  diff = similarity[sim_argsorted[-1]] - similarity[sim_argsorted[-2]]\n",
    "  if diff > THRESHOLD:\n",
    "    diff_datasets[sim_argsorted[-1]].append((similarity[sim_argsorted[-1]], texts))\n",
    "\n",
    "pseudo_texts = list()\n",
    "pseudo_labels = list()\n",
    "for i in range(len(classes)):\n",
    "  sorted_diff_data = sorted(diff_datasets[i], reverse=True)[:MAXLEN_GET_PSEUDO]\n",
    "  pseudo_texts.extend([i[1] for i in sorted_diff_data])\n",
    "  pseudo_labels.extend([i]*len(sorted_diff_data[:MAXLEN_GET_PSEUDO]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all selected data\n",
      "Com. : 24\n",
      "Edu. : 7\n",
      "Art. : 0\n",
      "Ath. : 1\n",
      "Off. : 124\n",
      "Mea. : 118\n",
      "Bui. : 2\n",
      "Nat. : 20\n",
      "Vil. : 1\n",
      "Ani. : 10\n",
      "Pla. : 1\n",
      "Alb. : 3\n",
      "Fil. : 3\n",
      "Wri. : 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of all selected data\")\n",
    "for i in diff_datasets:\n",
    "  print(classes[i][:3]+\". : \"+str(len(diff_datasets[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 50051.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "# dbpedia datasets train\n",
    "with open('../data/dbpedia_csv/test.csv','r',encoding='utf-8') as f:\n",
    "    reader = [r for r in csv.reader(f)]\n",
    "    \n",
    "# example -------------------\n",
    "import random\n",
    "reader = random.sample(reader, 100)\n",
    "#----------------------------\n",
    "\n",
    "test_texts = list()\n",
    "test_labels = list()\n",
    "for labels, auth, text in tqdm(reader):\n",
    "    text = text.replace(auth,'')\n",
    "    test_texts.append(preprocessing(text))\n",
    "    test_labels.append(int(labels)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb68d03a738c499e92311911c97f2876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55112c214af4ea1a9d2bfacaf856539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 324\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_dataset = datasets.Dataset.from_dict({\"text\":pseudo_texts, \"label\":pseudo_labels})\n",
    "test_dataset = datasets.Dataset.from_dict({\"text\":test_texts, \"label\":test_labels})\n",
    "dataset = datasets.DatasetDict({\"train\":train_dataset, \"test\":test_dataset})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, return_tensors=\"pt\", padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns('text')\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42) #.select(range(5000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42) #.select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=14)\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return evaluate.load(\"accuracy\").compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=SAVED_MODEL,\n",
    "  num_train_epochs=EPOCH,\n",
    "  per_device_train_batch_size=BATCH_SIZE,\n",
    "  per_device_eval_batch_size=BATCH_SIZE,\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  save_strategy=\"no\",\n",
    "  optim=\"adamw_torch\",\n",
    "  report_to=\"none\"\n",
    "  )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 324\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 81\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5620a3220a4046847e327b0a4f83d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49308250455b481ba636d59037025a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.151939630508423, 'eval_accuracy': 0.12, 'eval_runtime': 5.7032, 'eval_samples_per_second': 17.534, 'eval_steps_per_second': 4.383, 'epoch': 1.0}\n",
      "{'train_runtime': 46.2391, 'train_samples_per_second': 7.007, 'train_steps_per_second': 1.752, 'train_loss': 2.0594495608482832, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=81, training_loss=2.0594495608482832, metrics={'train_runtime': 46.2391, 'train_samples_per_second': 7.007, 'train_steps_per_second': 1.752, 'train_loss': 2.0594495608482832, 'epoch': 1.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../model/Proposed-Word2Vec-GPT2_20221125150654\\config.json\n",
      "Model weights saved in ../model/Proposed-Word2Vec-GPT2_20221125150654\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../model/Proposed-Word2Vec-GPT2_20221125150654\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"../model/Proposed-Word2Vec-GPT2_20221125150654\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2ForSequenceClassification\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file ../model/Proposed-Word2Vec-GPT2_20221125150654\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2ForSequenceClassification.\n",
      "\n",
      "All the weights of GPT2ForSequenceClassification were initialized from the model checkpoint at ../model/Proposed-Word2Vec-GPT2_20221125150654.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2ForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(SAVED_MODEL)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=SAVED_MODEL,report_to=\"none\")\n",
    "trainer = Trainer(model=model, args=training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e225a839fd4c15a1a41e4fdf446c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = trainer.predict(small_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Com.      0.000     0.000     0.000         8\n",
      "        Edu.      0.000     0.000     0.000         8\n",
      "        Art.      0.000     0.000     0.000         7\n",
      "        Ath.      0.000     0.000     0.000         7\n",
      "        Off.      0.222     0.600     0.324        10\n",
      "        Mea.      0.086     0.455     0.145        11\n",
      "        Bui.      0.000     0.000     0.000        10\n",
      "        Nat.      0.000     0.000     0.000         2\n",
      "        Vil.      0.000     0.000     0.000         5\n",
      "        Ani.      0.000     0.000     0.000        13\n",
      "        Pla.      0.000     0.000     0.000         3\n",
      "        Alb.      0.000     0.000     0.000         7\n",
      "        Fil.      0.071     0.250     0.111         4\n",
      "        Wri.      0.000     0.000     0.000         5\n",
      "\n",
      "    accuracy                          0.120       100\n",
      "   macro avg      0.027     0.093     0.041       100\n",
      "weighted avg      0.035     0.120     0.053       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\akasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\akasa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = [np.argmax(i) for i in pred.predictions]\n",
    "\n",
    "target_names = [c[:3]+\".\" for c in classes]\n",
    "\n",
    "rep = classification_report(pred.label_ids, y_pred, target_names=target_names, digits=3)\n",
    "print(rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e32349538976b425bbf8209bec3a52ef38eb988b8b568d4d0fb100f86dbbec2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
